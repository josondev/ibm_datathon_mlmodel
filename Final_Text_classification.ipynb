{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6a2932f-9c3f-4923-b765-615024b5bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9f79fec-e7d0-4def-9a47-d49a940b59c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(\"preprocessed-50k.csv\")\n",
    "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Remove null values and create dataframes\n",
    "x = train_data.dropna()\n",
    "X_train = x['text']\n",
    "y_train = x['source']\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['source']\n",
    "\n",
    "# Initialize TfidfVectorizer for nlp purposes\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "# Encoding the target variable necessary to convert the o/p in numerical form\n",
    "encoder = LabelEncoder()\n",
    "y_train_transformed = encoder.fit_transform(y_train)\n",
    "y_test_transformed = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed182d21-5bdb-430e-957b-e9784a619bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Remove null values and create dataframes\n",
    "x = train_data.dropna()\n",
    "X_train = x['text']\n",
    "y_train = x['source']\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['source']\n",
    "\n",
    "# Initialize TfidfVectorizer for nlp purposes\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_transformed = vectorizer.fit_transform(X_train)\n",
    "X_test_transformed = vectorizer.transform(X_test)\n",
    "\n",
    "# Encoding the target variable necessary to convert the o/p in numerical form\n",
    "encoder = LabelEncoder()\n",
    "y_train_transformed = encoder.fit_transform(y_train)\n",
    "y_test_transformed = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6bb399a-87bc-4223-86e4-766a87718665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.674706 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 568331\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691748\n",
      "[LightGBM] [Info] Start training from score -0.694548\n",
      "LightGBM Accuracy: 0.8268\n"
     ]
    }
   ],
   "source": [
    "#4. lightgbm is basically a gradient boosting framework just like adaboost or xgboost\n",
    "lgb_model = lgb.LGBMClassifier(objective='multiclass', num_class=len(encoder.classes_), n_estimators=100, random_state=42)\n",
    "lgb_model.fit(X_train_transformed, y_train_transformed)\n",
    "y_pred_lgb = lgb_model.predict(X_test_transformed)\n",
    "accuracy_lgb = accuracy_score(y_test_transformed, y_pred_lgb)\n",
    "print(\"LightGBM Accuracy:\", accuracy_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc1f63-5655-4da0-a175-cbaea1866f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1458 candidates, totalling 7290 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/jovyan/.local/lib/python3.10/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.449882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492125\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.983589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 493036\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4994\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.021483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492741\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.987472 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492639\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.990735 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492738\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.825721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492125\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.014948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 493036\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4994\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.993496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492741\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.209381 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492639\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.301967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492738\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.122874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492125\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.050162 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 493036\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4994\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.795922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492741\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.029424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492639\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.859244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492738\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.790844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492125\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.031511 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 493036\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4994\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.177630 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492741\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.366885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492639\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.993227 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492738\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.011398 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492125\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.046456 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 493036\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4994\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.012017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492741\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.984709 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492639\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.033792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492738\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.996421 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492125\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.173185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 493036\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4994\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.062415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492741\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.171326 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492639\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.994008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492738\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.963671 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492125\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.973079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 493036\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4994\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.003012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492741\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.978188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492639\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691711\n",
      "[LightGBM] [Info] Start training from score -0.694586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.026189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492738\n",
      "[LightGBM] [Info] Number of data points in the train set: 32000, number of used features: 4995\n",
      "[LightGBM] [Info] Start training from score -0.691773\n",
      "[LightGBM] [Info] Start training from score -0.694523\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.228481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 492125\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],         # Number of leaves in one tree (higher can increase complexity)\n",
    "    'learning_rate': [0.01, 0.05, 0.1],  # Step size shrinkage to prevent overfitting\n",
    "    'n_estimators': [100, 200, 300],     # Number of boosting iterations\n",
    "    'max_depth': [-1, 10, 20],           # Maximum depth of a tree\n",
    "    'min_child_samples': [20, 50],       # Minimum number of data points in a leaf\n",
    "    'subsample': [0.7, 0.8, 1.0],        # Subsample ratio of the training data\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]  # Subsample ratio of columns (features) when constructing each tree\n",
    "}\n",
    "\n",
    "# Apply GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(lgb_model, param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train_transformed, y_train_transformed)\n",
    "\n",
    "# Train the best model\n",
    "best_lgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lgb = best_lgb_model.predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model's accuracy\n",
    "accuracy_lgb = accuracy_score(y_test_transformed, y_pred_lgb)\n",
    "print(\"Best LightGBM Accuracy:\", accuracy_lgb)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_transformed, y_pred_lgb, target_names=encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea4a1045-5459-4616-a3ec-87dedd8a0fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ai       0.80      0.86      0.83      4972\n",
      "       human       0.85      0.79      0.82      5028\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "Saving model and components...\n",
      "\n",
      "Model training and evaluation complete. You can now classify text.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text you want to classify (or 'quit' to exit):  It looks like your system doesn't recognize the jupyter command, which usually means Jupyter Notebook isn't installed or the PATH environment variable isn't set up correctly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification: human\n",
      "This text appears to be human-generated.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text you want to classify (or 'quit' to exit):  To assist you with this task, I'll guide you through the process, as I cannot directly interact with external sites like LeetCode. Here's how you can proceed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification: ai\n",
      "This text appears to be ai-generated.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text you want to classify (or 'quit' to exit):  Artificial intelligence has revolutionized many industries, but it's also raised concerns about job displacement and ethical implications. As technology advances, finding a balance between innovation and human-centric policies will be essential.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification: ai\n",
      "This text appears to be ai-generated.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text you want to classify (or 'quit' to exit):  I was on my way to the grocery store when I saw an old friend from high school. We stopped to catch up for a bit, reminiscing about the times we spent studying for exams together. It's funny how time flies and we end up in such different places in life.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification: human\n",
      "This text appears to be human-generated.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text you want to classify (or 'quit' to exit):  The process of machine learning involves feeding large datasets into algorithms that can learn patterns and make predictions. Over time, the model improves its accuracy as it processes more data, but challenges still remain in terms of interpretability and bias.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification: ai\n",
      "This text appears to be ai-generated.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text you want to classify (or 'quit' to exit):  When I traveled to Japan, I was struck by how different everything was from my home country. The food was incredible, the people were so polite, and the culture was fascinating. I hope to go back one day and explore even more of the beautiful countryside.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification: human\n",
      "This text appears to be human-generated.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text you want to classify (or 'quit' to exit):  In a world where digital transformation is inevitable, organizations must adapt to emerging technologies or risk falling behind. The adoption of AI-driven solutions offers unprecedented opportunities for growth, yet companies must also consider cybersecurity risks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification: ai\n",
      "This text appears to be ai-generated.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text you want to classify (or 'quit' to exit):  To assist you with this task, I'll guide you through the process, as I cannot directly interact with external sites like LeetCode. Here's how you can proceed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification: ai\n",
      "This text appears to be ai-generated.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the text you want to classify (or 'quit' to exit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for using the AI Text Classifier. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "if('__main__'):\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_transformed, y_pred_lgb, target_names=encoder.classes_))\n",
    "    \n",
    "    # Save the model and necessary components\n",
    "    print(\"Saving model and components...\")\n",
    "    joblib.dump(lgb_model, 'lightgbm_model.joblib')\n",
    "    joblib.dump(vectorizer, 'tfidf_vectorizer.joblib')\n",
    "    joblib.dump(encoder, 'label_encoder.joblib')\n",
    "    \n",
    "    # Function to classify new text\n",
    "    def classify_text(text):\n",
    "        vectorized_text = vectorizer.transform([text])\n",
    "        prediction = lgb_model.predict(vectorized_text)\n",
    "        return encoder.inverse_transform(prediction)[0]\n",
    "    \n",
    "    # User interaction loop\n",
    "    print(\"\\nModel training and evaluation complete. You can now classify text.\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter the text you want to classify (or 'quit' to exit): \")\n",
    "    \n",
    "        if user_input.lower() == 'quit':\n",
    "            print(\"Thank you for using the AI Text Classifier. Goodbye!\")\n",
    "            break\n",
    "    \n",
    "        classification = classify_text(user_input)\n",
    "        print(f\"\\nClassification: {classification}\")\n",
    "    \n",
    "        # Use the actual label names from your dataset\n",
    "        if classification == encoder.classes_[0]:  # Assuming 0 index is for human-generated\n",
    "            print(f\"This text appears to be {encoder.classes_[0]}-generated.\")\n",
    "        else:\n",
    "            print(f\"This text appears to be {encoder.classes_[1]}-generated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
